{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3663bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install lxml\n",
    "#!C:\\ProgramData\\anaconda3\\python.exe -m pip install seleniumbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a420a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Scraping class for various scraping-related functions.\n",
    "\n",
    "Methods:\n",
    "    __init__: Initialize the Scraping object.\n",
    "    __del__: Destructor for the Scraping object.\n",
    "    encode_code: Encode code as base64.\n",
    "    decode_code: Decode base64-encoded code.\n",
    "    decode_picture: Decode base64-encoded picture.\n",
    "    get_result_meta: Get metadata for a given URL.\n",
    "    take_screenshot: Take a screenshot of the browser window.\n",
    "    get_real_url: Get the real URL after any redirects.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib import request\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "import urllib.parse\n",
    "import socket\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time\n",
    "\n",
    "class Scraping:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Scraping object.\n",
    "        \"\"\"        \n",
    "        self = self\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        Destructor for the Scraping object.\n",
    "        \"\"\"        \n",
    "        print('Helper object destroyed')\n",
    "\n",
    "    def encode_code(self, code):\n",
    "        \"\"\"\n",
    "        Encode code as base64.\n",
    "\n",
    "        Args:\n",
    "            code (str): Code to encode.\n",
    "\n",
    "        Returns:\n",
    "            str: Base64-encoded code.\n",
    "        \"\"\"        \n",
    "        code = code.encode('utf-8','ignore')\n",
    "        code = base64.b64encode(code)\n",
    "        return code\n",
    "\n",
    "    def decode_code(self, value):\n",
    "        \"\"\"\n",
    "        Decode base64-encoded code.\n",
    "\n",
    "        Args:\n",
    "            value (str): Base64-encoded code.\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded code.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            code_decoded = base64.b64decode(value)\n",
    "            code_decoded = BeautifulSoup(code_decoded, \"html.parser\")\n",
    "            code_decoded = str(code_decoded)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            code_decoded = \"decoding error\"\n",
    "        return code_decoded\n",
    "\n",
    "\n",
    "\n",
    "    def decode_picture(self, value):\n",
    "        \"\"\"\n",
    "        Decode base64-encoded picture.\n",
    "\n",
    "        Args:\n",
    "            value (str): Base64-encoded picture.\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded picture.\n",
    "        \"\"\"        \n",
    "        picture = value.tobytes()\n",
    "        picture = picture.decode('ascii')\n",
    "        return picture\n",
    "\n",
    "    def get_result_meta(self, url):\n",
    "        \"\"\"\n",
    "        Get metadata for a given URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL to get metadata for.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the metadata.\n",
    "        \"\"\"        \n",
    "        meta = {}\n",
    "        ip = \"-1\"\n",
    "        main = url\n",
    "        #parse url to get hostname and socket\n",
    "        try:\n",
    "            parsed_uri = urlparse(url)\n",
    "            hostname = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "            ip = socket.gethostbyname(hostname)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            ip = \"-1\"\n",
    "\n",
    "        try:\n",
    "            main = '{0.scheme}://{0.netloc}/'.format(urlsplit(url))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            main = url\n",
    "\n",
    "        #write to meta dictionary\n",
    "        meta = {\"ip\":ip, \"main\":main}\n",
    "\n",
    "        return meta\n",
    "\n",
    "\n",
    "\n",
    "    def take_screenshot(self, driver):\n",
    "        \"\"\"\n",
    "        Take a screenshot of the browser window.\n",
    "\n",
    "        Args:\n",
    "            driver: WebDriver instance.\n",
    "\n",
    "        Returns:\n",
    "            str: Base64-encoded screenshot image.\n",
    "        \"\"\"\n",
    "        #function to encode file content to base64\n",
    "        def encode_file_base64(self, file):\n",
    "            f = open(file, 'rb')\n",
    "            code = f.read()\n",
    "            code = base64.b64encode(code)\n",
    "            f.close()\n",
    "            return code\n",
    "\n",
    "        current_path = os.path.abspath(os.getcwd())\n",
    "\n",
    "        #iniatilize constant variables\n",
    "\n",
    "        #iniatilize the directories for the extension and for the folder for temporary downlods of files\n",
    "        if os.name == \"nt\":\n",
    "            screenshot_folder = current_path+\"\\\\tmp\\\\\"\n",
    "\n",
    "\n",
    "        else:\n",
    "            screenshot_folder = current_path+\"//tmp//\"\n",
    "\n",
    "        screenshot_file = screenshot_folder+str(uuid.uuid1())+\".png\"\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.maximize_window() #maximize browser window for screenshot\n",
    "        driver.save_screenshot(screenshot_file)\n",
    "\n",
    "        # #open screenshot and save as base64\n",
    "        screenshot = encode_file_base64(self, screenshot_file)\n",
    "\n",
    "        os.remove(screenshot_file)\n",
    "\n",
    "        return screenshot #return base64 code of image\n",
    "\n",
    "    def get_real_url(url, driver):\n",
    "        \"\"\"\n",
    "        Get the real URL after any redirects.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL to get the real URL for.\n",
    "            driver: WebDriver instance.\n",
    "\n",
    "        Returns:\n",
    "            str: Real URL after any redirects.\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(4)\n",
    "            current_url = driver.current_url #read real url (redirected url)\n",
    "            driver.quit()\n",
    "            return current_url\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcc3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This template provides a framework for creating a custom scraper for the RAT software. This scraper is designed to work with search services that offer search forms. For other types of search systems, modifications to this template may be necessary. Selenium is utilized as the primary tool for web scraping.\n",
    "\n",
    "The scraper should be capable of returning the following fields:\n",
    "- `result_title`: The title of the search result snippet.\n",
    "- `result_description`: The description in the snippet of the result.\n",
    "- `result_url`: The URL of the search result.\n",
    "- `serp_code`: The HTML source code of the search result page, useful for further analysis.\n",
    "- `serp_bin`: A screenshot of the search result page, if needed for additional analysis.\n",
    "- `page`: The page number of search results, useful for paginated results or scrolling-based systems.\n",
    "\n",
    "A typical scraper consists of the following functions:\n",
    "- `run(query, limit, scraping, headless)`: The main function to execute the scraper with the given parameters.\n",
    "- `get_search_results(driver, page)`: A helper function to retrieve search results from the given page.\n",
    "- `check_captcha(driver)`: A helper function to check for CAPTCHA or similar blocks and handle them appropriately.\n",
    "\n",
    "The variables and functionality described here can be adapted according to the specific search engine being scraped.\n",
    "\n",
    "The search engine in this template is Ecosia. Change the parameters according to the search engine you want to scrape.\n",
    "\"\"\"\n",
    "\n",
    "#library with functions for web scraping\n",
    "\n",
    "#import external libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException #used to interrupt loding of websites and needed as workaround to download files with selenium\n",
    "from selenium.webdriver.common.action_chains import ActionChains #used to simulate pressing of a key\n",
    "\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "\n",
    "import uuid #used to generate random file names\n",
    "\n",
    "import time #used to do timeout breaks\n",
    "\n",
    "import os #used for file management\n",
    "\n",
    "#base64 encoding to convert the code codes of webpages\n",
    "import base64\n",
    "\n",
    "#BeautifulSoup is necessary to beautify the code coded after it has been decoded (especially useful to prevent character errors)\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "import re\n",
    "\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "parentdir = os.path.dirname(parentdir)\n",
    "\n",
    "ext_path = parentdir+\"/i_care_about_cookies_unpacked\"\n",
    "\n",
    "from seleniumbase import Driver\n",
    "\n",
    "def run(query, limit, scraping, headless):\n",
    "    \"\"\"\n",
    "    Run the scraper.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        limit (int): The maximum number of search results to retrieve.\n",
    "        scraping: The Scraping object.\n",
    "        headless (bool): If True, runs the browser in headless mode (without GUI).\n",
    "\n",
    "    Returns:\n",
    "        list: List of search results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL and selectors for the search engine\n",
    "        search_url = \"https://www.ecosia.org/\" #URL for the search engine\n",
    "        search_box = \"q\" #Selector for the search box\n",
    "        captcha = \"g-recaptcha\" #Selector for CAPTCHA in the page source\n",
    "\n",
    "        # Initialize variables\n",
    "        results_number = 0 #Initialize number of search results\n",
    "        page = -1 #Initialize SERP page number\n",
    "        search_results = [] #Initialize list of search results\n",
    "        \n",
    "        # Custom function to scrape search results\n",
    "        def get_search_results(driver, page):\n",
    "            \"\"\"\n",
    "            Retrieve search results from the current page.\n",
    "\n",
    "            Args:\n",
    "                driver: Selenium WebDriver instance.\n",
    "                page (int): Current SERP page.\n",
    "\n",
    "            Returns:\n",
    "                list: List of search results from the current page.\n",
    "            \"\"\"\n",
    "            temp_search_results = []\n",
    "\n",
    "            # Get page source and encode it\n",
    "            source = driver.page_source\n",
    "            serp_code = scraping.encode_code(source)\n",
    "            serp_bin = scraping.take_screenshot(driver)\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(source, features=\"lxml\")\n",
    "\n",
    "            # Extract search results using CSS selectors\n",
    "            for result in soup.find_all(\"div\", class_=[\"result__body\"]):\n",
    "                result_title = \"N/A\" #Initialize result title\n",
    "                result_description = \"N/A\" #Initialize result description\n",
    "                result_url = \"N/A\" #Initialize result URL\n",
    "\n",
    "                try:\n",
    "                    title_elem = result.find(\"div\", class_=[\"result__title\"])\n",
    "                    if title_elem:\n",
    "                        result_title = title_elem.text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    description_elem = result.find(\"div\", class_=[\"result__description\"])\n",
    "                    if description_elem:\n",
    "                        result_description = description_elem.text.strip()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    url_elem = result.find(\"a\")\n",
    "                    if url_elem:\n",
    "                        url = url_elem.attrs['href']\n",
    "                        if \"bing.\" in url:\n",
    "                            url = scraping.get_real_url(url)\n",
    "                        result_url = url\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                if result_url != \"N/A\":\n",
    "                    temp_search_results.append([result_title, result_description, result_url, serp_code, serp_bin, page])\n",
    "\n",
    "            return temp_search_results\n",
    "\n",
    "        # Custom function to check if CAPTCHA is present\n",
    "        def check_captcha(driver):\n",
    "            \"\"\"\n",
    "            Check if CAPTCHA is present on the page.\n",
    "\n",
    "            Args:\n",
    "                driver: Selenium WebDriver instance.\n",
    "\n",
    "            Returns:\n",
    "                bool: True if CAPTCHA is present, False otherwise.\n",
    "            \"\"\"\n",
    "            source = driver.page_source\n",
    "            return captcha in source\n",
    "        \n",
    "        def remove_duplicates(search_results):\n",
    "            \"\"\"\n",
    "            Removes duplicate search results based on the URL.\n",
    "\n",
    "            Args:\n",
    "                search_results (list): List of search results to deduplicate.\n",
    "\n",
    "            Returns:\n",
    "                list: List of search results with duplicates removed.\n",
    "            \"\"\"\n",
    "            seen_urls = set()\n",
    "            unique_results = []\n",
    "\n",
    "            # Append only unique results\n",
    "            for result in search_results:\n",
    "                url = result[2]\n",
    "                if url not in seen_urls:\n",
    "                    seen_urls.add(url)\n",
    "                    unique_results.append(result)\n",
    "\n",
    "            return unique_results        \n",
    "\n",
    "        # Initialize Selenium driver\n",
    "        driver = Driver(\n",
    "            browser=\"chrome\",\n",
    "            wire=True,\n",
    "            uc=True,\n",
    "            headless2=headless,\n",
    "            incognito=False,\n",
    "            agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "            do_not_track=True,\n",
    "            undetectable=True,\n",
    "            extension_dir=ext_path,\n",
    "            locale_code=\"de\"\n",
    "        )\n",
    "\n",
    "        driver.maximize_window()\n",
    "        driver.set_page_load_timeout(20)\n",
    "        driver.implicitly_wait(30)\n",
    "        driver.get(search_url)\n",
    "        time.sleep(random.randint(2, 5))\n",
    "\n",
    "        # Start scraping if no CAPTCHA\n",
    "        if not check_captcha(driver):\n",
    "            search = driver.find_element(By.NAME, search_box) #Find search box\n",
    "            search.send_keys(query) #Enter search query\n",
    "            search.send_keys(Keys.RETURN) #Submit search\n",
    "            time.sleep(random.randint(2, 5)) #Wait for Results\n",
    "\n",
    "            search_results = get_search_results(driver, page)\n",
    "            results_number = len(search_results)\n",
    "            continue_scraping = True #Initialize scraping\n",
    "\n",
    "            # Loop through pages until limit is reached or CAPTCHA appears\n",
    "            while results_number < limit and continue_scraping:\n",
    "                if not check_captcha(driver):\n",
    "                    time.sleep(random.randint(2, 5))\n",
    "                    page += 1\n",
    "                    try:\n",
    "                        next_page_url = f\"https://www.ecosia.org/search?method=index&q={query}&p={page}\" #Next page URL\n",
    "                        print(next_page_url)\n",
    "                        driver.get(next_page_url)\n",
    "                        extract_search_results = get_search_results(driver, page)\n",
    "                        print(f\"Results extracted: {len(extract_search_results)}\")\n",
    "\n",
    "                        if extract_search_results:\n",
    "                            print(\"Appending results.\")\n",
    "                            search_results += extract_search_results\n",
    "                            search_results = remove_duplicates(search_results)\n",
    "                            results_number = len(search_results)\n",
    "                        else:\n",
    "                            continue_scraping = False\n",
    "                            search_results = -1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to get next page: {e}\")\n",
    "                        continue_scraping = False\n",
    "                else:\n",
    "                    continue_scraping = False\n",
    "                    search_results = -1\n",
    "\n",
    "            driver.quit()\n",
    "            return search_results\n",
    "        else:\n",
    "            search_results = -1\n",
    "            driver.quit()\n",
    "            return search_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cd7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the scraper. it shows all scraped details or an error message, if it fails\n",
    "\n",
    "def test_scraper(query, limit, scraper, headless):\n",
    "    search_results = run(query, limit, scraper, headless)\n",
    "\n",
    "    i = 0\n",
    "    if search_results != -1:\n",
    "        for sr in search_results:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            print(sr[0])\n",
    "            print(sr[1])\n",
    "            print(sr[2])\n",
    "    else:\n",
    "        print(\"Scraping failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bc2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the scraper: Change the parameters for testing your scraper\n",
    "scraper = Scraping() #initialize the scraping object\n",
    "\n",
    "query = \"politik\" #search query\n",
    "limit = 10 #max_number of results (the scraper normally adds some more pages since not all search engines deliver a certain number of search results on every SERP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0389e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: uc_driver not found. Getting it now:\n",
      "\n",
      "*** chromedriver to download = 127.0.6533.88 (Latest Stable) \n",
      "\n",
      "Downloading chromedriver-win64.zip from:\n",
      "https://storage.googleapis.com/chrome-for-testing-public/127.0.6533.88/win64/chromedriver-win64.zip ...\n",
      "Download Complete!\n",
      "\n",
      "Extracting ['chromedriver.exe'] from chromedriver-win64.zip ...\n",
      "Unzip Complete!\n",
      "\n",
      "The file [uc_driver.exe] was saved to:\n",
      "C:\\Users\\stahl\\AppData\\Roaming\\Python\\Python312\\site-packages\\seleniumbase\\drivers\\uc_driver.exe\n",
      "\n",
      "Making [uc_driver.exe 127.0.6533.88] executable ...\n",
      "[uc_driver.exe 127.0.6533.88] is now ready for use!\n",
      "\n",
      "1\n",
      "Politik: Aktuelle Nachrichten aus dem In- und Ausland - FAZ.NET\n",
      "vor 6 Stunden ... Politische Bücher: Gelebte Verfassung. Das Grundgesetz ist 75 Jahre alt. Wie weit stiftet das einstige Provisorium Identität? Mehrere Autoren denken über den „ ...\n",
      "https://www.faz.net/aktuell/politik/\n",
      "2\n",
      "Politik News: Aktuelle Nachrichten im Überblick - ZDFheute\n",
      "Aktuelle Nachrichten aus der Politik · Liveblog:Aktuelle Nachrichten zur Eskalation in Nahost · Harris versus Trump:Aktuelles zum US-Präsidentschaftswahlkampf.\n",
      "https://www.zdf.de/nachrichten/politik\n",
      "3\n",
      "Politik - Aktuelle Nachrichten & News - WELT\n",
      "vor 6 Stunden ... Die zwei Nato-Länder, die jetzt schon ihre Kontakte zu Donald Trump aktivieren. Viele Europäer sind angesichts der Aussicht einer erneuten Trump-Präsidentschaft ...\n",
      "https://www.welt.de/politik/\n",
      "4\n",
      "Politik - DER SPIEGEL\n",
      "vor 6 Stunden ... Sonntagsfrage Ampelkoalition ist unbeliebter als je zuvor · Wäre heute Bundestagswahl, würden nur noch zehn Prozent der Wähler die Grünen wählen. Eine Insa- ...\n",
      "https://www.spiegel.de/politik/\n",
      "5\n",
      "Politik - aktuelle Nachrichten und Reportagen | ZEIT ONLINE\n",
      "vor 5 Stunden ... Gefangenenaustausch : Trump gratuliert Putin zu \"großartigem\" Gefangenendeal. Bei einem Wahlkampfauftritt lobt Präsidentschaftskandidat Donald Trump die ...\n",
      "https://www.zeit.de/politik/index\n",
      "6\n",
      "Politik-News: Aktuelle Nachrichten aus Deutschland und der ...\n",
      "vor 4 Stunden ... Gemeinde Neulingen in Baden-Württemberg: Polizei stoppt Lesung von Rechtsextremist Sellner und spricht Aufenthaltsverbot aus. Die Maßnahmen hätten der ...\n",
      "https://www.tagesspiegel.de/politik/\n",
      "7\n",
      "Politik aktuell - Nachrichten aus Deutschland und der Welt\n",
      "vor 8 Stunden ... Das Politische Buch · Memoiren. :Ein Leben im Vollen · Das Politische Buch. :Kein Grund zur Panik · Das Politische Buch. :Protokoll eines Versagens · Naher ...\n",
      "https://www.sueddeutsche.de/politik\n",
      "8\n",
      "Politik aktuell: Nachrichten aus Deutschland, Europa und der ...\n",
      "Aktuelle News, Informationen und Videos zu Politik, Panorama und Wetter aus Deutschland, Europa und der Welt von t-online.de Nachrichten.\n",
      "https://www.t-online.de/nachrichten/\n",
      "9\n",
      "Die Politik der Bundesregierung | Tatsachen über Deutschland\n",
      "Er löste die CDU-Politikerin Angela Merkel ab, die Deutschland 16 Jahre regiert hatte. Die seither amtierende Bundesregierung Bundesregierung Bundeskanzler oder ...\n",
      "https://www.tatsachen-ueber-deutschland.de/de/politik-deutschland/politik-der-bundesregierung\n",
      "10\n",
      "Politik-Nachrichten aus Deutschland und dem Ausland | FR.de\n",
      "vor 9 Stunden ... Eskalation im Nahen Osten droht: Iran will Israel laut offiziellen Angaben angreifen. Der Iran und Israel überziehen sich mit Drohungen, ein regionaler ...\n",
      "https://www.fr.de/politik/\n"
     ]
    }
   ],
   "source": [
    "test_scraper(query, limit, scraper, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263056d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
